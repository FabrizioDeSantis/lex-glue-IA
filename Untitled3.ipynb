{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN2r0UdkkvgA9nPjv6vzKby",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FabrizioDeSantis/lex-glue-IA/blob/main/Untitled3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-__Pa252xhGm",
        "outputId": "90c2c283-bddd-4309-ea03-f80258751170"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:__main__:Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True\n",
            "INFO:__main__:Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=2,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=epoch,\n",
            "fp16=True,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=True,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=2,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=True,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=3e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=True,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=logs/multi_eurlex/xlm-roberta-base/seed_1/runs/Oct08_08-59-20_b132cc6577fb,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=micro-f1,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=2.0,\n",
            "optim=adamw_hf,\n",
            "output_dir=logs/multi_eurlex/xlm-roberta-base/seed_1,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=8,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=logs/multi_eurlex/xlm-roberta-base/seed_1,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=epoch,\n",
            "save_total_limit=5,\n",
            "seed=1,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "INFO:datasets.utils.file_utils:https://huggingface.co/datasets/multi_eurlex/resolve/main/multi_eurlex.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmp29wk0ukh\n",
            "Downloading builder script: 100% 138k/138k [00:00<00:00, 197kB/s] \n",
            "INFO:datasets.utils.file_utils:storing https://huggingface.co/datasets/multi_eurlex/resolve/main/multi_eurlex.py in cache at /root/.cache/huggingface/datasets/downloads/38f8abf57566d2b9b1c9369efd3415f5a8691c42392c12262f6435b20ed55bab.642a32ef088298747db464dca937d75b531e583d7e27a798c387f346d2b0c911.py\n",
            "INFO:datasets.utils.file_utils:creating metadata file for /root/.cache/huggingface/datasets/downloads/38f8abf57566d2b9b1c9369efd3415f5a8691c42392c12262f6435b20ed55bab.642a32ef088298747db464dca937d75b531e583d7e27a798c387f346d2b0c911.py\n",
            "INFO:datasets.utils.file_utils:https://huggingface.co/datasets/multi_eurlex/resolve/main/dataset_infos.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpw68xqc6n\n",
            "Downloading metadata: 100% 57.7k/57.7k [00:00<00:00, 124kB/s] \n",
            "INFO:datasets.utils.file_utils:storing https://huggingface.co/datasets/multi_eurlex/resolve/main/dataset_infos.json in cache at /root/.cache/huggingface/datasets/downloads/e60ab36f68ed3329af7eec9ea95c9d67f628929585bf59c3475624eeedd043f0.79935b7003e7892bcf51412a9143325386eb1d9c16aa046622c52bae9dd8947d\n",
            "INFO:datasets.utils.file_utils:creating metadata file for /root/.cache/huggingface/datasets/downloads/e60ab36f68ed3329af7eec9ea95c9d67f628929585bf59c3475624eeedd043f0.79935b7003e7892bcf51412a9143325386eb1d9c16aa046622c52bae9dd8947d\n",
            "INFO:datasets.info:Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/multi_eurlex/8ec8b79877a517369a143ead6679d1788d13e51cf641ed29772f4449e8364fb6\n",
            "INFO:datasets.builder:Generating dataset multi_eurlex (/root/.cache/huggingface/datasets/multi_eurlex/it/1.0.0/8ec8b79877a517369a143ead6679d1788d13e51cf641ed29772f4449e8364fb6)\n",
            "Downloading and preparing dataset multi_eurlex/it (download: 2.58 GiB, generated: 515.09 MiB, post-processed: Unknown size, total: 3.08 GiB) to /root/.cache/huggingface/datasets/multi_eurlex/it/1.0.0/8ec8b79877a517369a143ead6679d1788d13e51cf641ed29772f4449e8364fb6...\n",
            "INFO:datasets.builder:Dataset not on Hf google storage. Downloading and preparing it from source\n",
            "INFO:datasets.utils.file_utils:https://zenodo.org/record/5363165/files/multi_eurlex.tar.gz not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpie25mfj6\n",
            "Downloading data: 100% 2.77G/2.77G [05:48<00:00, 7.95MB/s]\n",
            "INFO:datasets.utils.file_utils:storing https://zenodo.org/record/5363165/files/multi_eurlex.tar.gz in cache at /root/.cache/huggingface/datasets/downloads/1aa0a9be39062a0f58ee69ed3f16500da085d83fbc1495decda3e23c3ae74b8e\n",
            "INFO:datasets.utils.file_utils:creating metadata file for /root/.cache/huggingface/datasets/downloads/1aa0a9be39062a0f58ee69ed3f16500da085d83fbc1495decda3e23c3ae74b8e\n",
            "INFO:datasets.download.download_manager:Downloading took 5.0 min\n",
            "INFO:datasets.download.download_manager:Checksum Computation took 0.0 min\n",
            "INFO:datasets.utils.info_utils:All the checksums matched successfully for dataset source files\n",
            "INFO:datasets.builder:Generating train split\n",
            "INFO:datasets.builder:Generating test split\n",
            "INFO:datasets.builder:Generating validation split\n",
            "INFO:datasets.utils.info_utils:All the splits matched successfully.\n",
            "Dataset multi_eurlex downloaded and prepared to /root/.cache/huggingface/datasets/multi_eurlex/it/1.0.0/8ec8b79877a517369a143ead6679d1788d13e51cf641ed29772f4449e8364fb6. Subsequent calls will reuse this data.\n",
            "INFO:datasets.info:Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/multi_eurlex/8ec8b79877a517369a143ead6679d1788d13e51cf641ed29772f4449e8364fb6\n",
            "INFO:datasets.builder:Overwrite dataset info from restored data version.\n",
            "INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/multi_eurlex/it/1.0.0/8ec8b79877a517369a143ead6679d1788d13e51cf641ed29772f4449e8364fb6\n",
            "WARNING:datasets.builder:Found cached dataset multi_eurlex (/root/.cache/huggingface/datasets/multi_eurlex/it/1.0.0/8ec8b79877a517369a143ead6679d1788d13e51cf641ed29772f4449e8364fb6)\n",
            "INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/multi_eurlex/it/1.0.0/8ec8b79877a517369a143ead6679d1788d13e51cf641ed29772f4449e8364fb6\n",
            "INFO:datasets.info:Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/multi_eurlex/8ec8b79877a517369a143ead6679d1788d13e51cf641ed29772f4449e8364fb6\n",
            "INFO:datasets.builder:Overwrite dataset info from restored data version.\n",
            "INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/multi_eurlex/it/1.0.0/8ec8b79877a517369a143ead6679d1788d13e51cf641ed29772f4449e8364fb6\n",
            "WARNING:datasets.builder:Found cached dataset multi_eurlex (/root/.cache/huggingface/datasets/multi_eurlex/it/1.0.0/8ec8b79877a517369a143ead6679d1788d13e51cf641ed29772f4449e8364fb6)\n",
            "INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/multi_eurlex/it/1.0.0/8ec8b79877a517369a143ead6679d1788d13e51cf641ed29772f4449e8364fb6\n",
            "Downloading: 100% 615/615 [00:00<00:00, 561kB/s]\n",
            "[INFO|configuration_utils.py:648] 2022-10-08 09:08:39,678 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/f6d161e8f5f6f2ed433fb4023d6cb34146506b3f/config.json\n",
            "[INFO|configuration_utils.py:700] 2022-10-08 09:08:39,695 >> Model config XLMRobertaConfig {\n",
            "  \"_name_or_path\": \"xlm-roberta-base\",\n",
            "  \"architectures\": [\n",
            "    \"XLMRobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"finetuning_task\": \"multi_eurlex\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\",\n",
            "    \"4\": \"LABEL_4\",\n",
            "    \"5\": \"LABEL_5\",\n",
            "    \"6\": \"LABEL_6\",\n",
            "    \"7\": \"LABEL_7\",\n",
            "    \"8\": \"LABEL_8\",\n",
            "    \"9\": \"LABEL_9\",\n",
            "    \"10\": \"LABEL_10\",\n",
            "    \"11\": \"LABEL_11\",\n",
            "    \"12\": \"LABEL_12\",\n",
            "    \"13\": \"LABEL_13\",\n",
            "    \"14\": \"LABEL_14\",\n",
            "    \"15\": \"LABEL_15\",\n",
            "    \"16\": \"LABEL_16\",\n",
            "    \"17\": \"LABEL_17\",\n",
            "    \"18\": \"LABEL_18\",\n",
            "    \"19\": \"LABEL_19\",\n",
            "    \"20\": \"LABEL_20\",\n",
            "    \"21\": \"LABEL_21\",\n",
            "    \"22\": \"LABEL_22\",\n",
            "    \"23\": \"LABEL_23\",\n",
            "    \"24\": \"LABEL_24\",\n",
            "    \"25\": \"LABEL_25\",\n",
            "    \"26\": \"LABEL_26\",\n",
            "    \"27\": \"LABEL_27\",\n",
            "    \"28\": \"LABEL_28\",\n",
            "    \"29\": \"LABEL_29\",\n",
            "    \"30\": \"LABEL_30\",\n",
            "    \"31\": \"LABEL_31\",\n",
            "    \"32\": \"LABEL_32\",\n",
            "    \"33\": \"LABEL_33\",\n",
            "    \"34\": \"LABEL_34\",\n",
            "    \"35\": \"LABEL_35\",\n",
            "    \"36\": \"LABEL_36\",\n",
            "    \"37\": \"LABEL_37\",\n",
            "    \"38\": \"LABEL_38\",\n",
            "    \"39\": \"LABEL_39\",\n",
            "    \"40\": \"LABEL_40\",\n",
            "    \"41\": \"LABEL_41\",\n",
            "    \"42\": \"LABEL_42\",\n",
            "    \"43\": \"LABEL_43\",\n",
            "    \"44\": \"LABEL_44\",\n",
            "    \"45\": \"LABEL_45\",\n",
            "    \"46\": \"LABEL_46\",\n",
            "    \"47\": \"LABEL_47\",\n",
            "    \"48\": \"LABEL_48\",\n",
            "    \"49\": \"LABEL_49\",\n",
            "    \"50\": \"LABEL_50\",\n",
            "    \"51\": \"LABEL_51\",\n",
            "    \"52\": \"LABEL_52\",\n",
            "    \"53\": \"LABEL_53\",\n",
            "    \"54\": \"LABEL_54\",\n",
            "    \"55\": \"LABEL_55\",\n",
            "    \"56\": \"LABEL_56\",\n",
            "    \"57\": \"LABEL_57\",\n",
            "    \"58\": \"LABEL_58\",\n",
            "    \"59\": \"LABEL_59\",\n",
            "    \"60\": \"LABEL_60\",\n",
            "    \"61\": \"LABEL_61\",\n",
            "    \"62\": \"LABEL_62\",\n",
            "    \"63\": \"LABEL_63\",\n",
            "    \"64\": \"LABEL_64\",\n",
            "    \"65\": \"LABEL_65\",\n",
            "    \"66\": \"LABEL_66\",\n",
            "    \"67\": \"LABEL_67\",\n",
            "    \"68\": \"LABEL_68\",\n",
            "    \"69\": \"LABEL_69\",\n",
            "    \"70\": \"LABEL_70\",\n",
            "    \"71\": \"LABEL_71\",\n",
            "    \"72\": \"LABEL_72\",\n",
            "    \"73\": \"LABEL_73\",\n",
            "    \"74\": \"LABEL_74\",\n",
            "    \"75\": \"LABEL_75\",\n",
            "    \"76\": \"LABEL_76\",\n",
            "    \"77\": \"LABEL_77\",\n",
            "    \"78\": \"LABEL_78\",\n",
            "    \"79\": \"LABEL_79\",\n",
            "    \"80\": \"LABEL_80\",\n",
            "    \"81\": \"LABEL_81\",\n",
            "    \"82\": \"LABEL_82\",\n",
            "    \"83\": \"LABEL_83\",\n",
            "    \"84\": \"LABEL_84\",\n",
            "    \"85\": \"LABEL_85\",\n",
            "    \"86\": \"LABEL_86\",\n",
            "    \"87\": \"LABEL_87\",\n",
            "    \"88\": \"LABEL_88\",\n",
            "    \"89\": \"LABEL_89\",\n",
            "    \"90\": \"LABEL_90\",\n",
            "    \"91\": \"LABEL_91\",\n",
            "    \"92\": \"LABEL_92\",\n",
            "    \"93\": \"LABEL_93\",\n",
            "    \"94\": \"LABEL_94\",\n",
            "    \"95\": \"LABEL_95\",\n",
            "    \"96\": \"LABEL_96\",\n",
            "    \"97\": \"LABEL_97\",\n",
            "    \"98\": \"LABEL_98\",\n",
            "    \"99\": \"LABEL_99\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_10\": 10,\n",
            "    \"LABEL_11\": 11,\n",
            "    \"LABEL_12\": 12,\n",
            "    \"LABEL_13\": 13,\n",
            "    \"LABEL_14\": 14,\n",
            "    \"LABEL_15\": 15,\n",
            "    \"LABEL_16\": 16,\n",
            "    \"LABEL_17\": 17,\n",
            "    \"LABEL_18\": 18,\n",
            "    \"LABEL_19\": 19,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_20\": 20,\n",
            "    \"LABEL_21\": 21,\n",
            "    \"LABEL_22\": 22,\n",
            "    \"LABEL_23\": 23,\n",
            "    \"LABEL_24\": 24,\n",
            "    \"LABEL_25\": 25,\n",
            "    \"LABEL_26\": 26,\n",
            "    \"LABEL_27\": 27,\n",
            "    \"LABEL_28\": 28,\n",
            "    \"LABEL_29\": 29,\n",
            "    \"LABEL_3\": 3,\n",
            "    \"LABEL_30\": 30,\n",
            "    \"LABEL_31\": 31,\n",
            "    \"LABEL_32\": 32,\n",
            "    \"LABEL_33\": 33,\n",
            "    \"LABEL_34\": 34,\n",
            "    \"LABEL_35\": 35,\n",
            "    \"LABEL_36\": 36,\n",
            "    \"LABEL_37\": 37,\n",
            "    \"LABEL_38\": 38,\n",
            "    \"LABEL_39\": 39,\n",
            "    \"LABEL_4\": 4,\n",
            "    \"LABEL_40\": 40,\n",
            "    \"LABEL_41\": 41,\n",
            "    \"LABEL_42\": 42,\n",
            "    \"LABEL_43\": 43,\n",
            "    \"LABEL_44\": 44,\n",
            "    \"LABEL_45\": 45,\n",
            "    \"LABEL_46\": 46,\n",
            "    \"LABEL_47\": 47,\n",
            "    \"LABEL_48\": 48,\n",
            "    \"LABEL_49\": 49,\n",
            "    \"LABEL_5\": 5,\n",
            "    \"LABEL_50\": 50,\n",
            "    \"LABEL_51\": 51,\n",
            "    \"LABEL_52\": 52,\n",
            "    \"LABEL_53\": 53,\n",
            "    \"LABEL_54\": 54,\n",
            "    \"LABEL_55\": 55,\n",
            "    \"LABEL_56\": 56,\n",
            "    \"LABEL_57\": 57,\n",
            "    \"LABEL_58\": 58,\n",
            "    \"LABEL_59\": 59,\n",
            "    \"LABEL_6\": 6,\n",
            "    \"LABEL_60\": 60,\n",
            "    \"LABEL_61\": 61,\n",
            "    \"LABEL_62\": 62,\n",
            "    \"LABEL_63\": 63,\n",
            "    \"LABEL_64\": 64,\n",
            "    \"LABEL_65\": 65,\n",
            "    \"LABEL_66\": 66,\n",
            "    \"LABEL_67\": 67,\n",
            "    \"LABEL_68\": 68,\n",
            "    \"LABEL_69\": 69,\n",
            "    \"LABEL_7\": 7,\n",
            "    \"LABEL_70\": 70,\n",
            "    \"LABEL_71\": 71,\n",
            "    \"LABEL_72\": 72,\n",
            "    \"LABEL_73\": 73,\n",
            "    \"LABEL_74\": 74,\n",
            "    \"LABEL_75\": 75,\n",
            "    \"LABEL_76\": 76,\n",
            "    \"LABEL_77\": 77,\n",
            "    \"LABEL_78\": 78,\n",
            "    \"LABEL_79\": 79,\n",
            "    \"LABEL_8\": 8,\n",
            "    \"LABEL_80\": 80,\n",
            "    \"LABEL_81\": 81,\n",
            "    \"LABEL_82\": 82,\n",
            "    \"LABEL_83\": 83,\n",
            "    \"LABEL_84\": 84,\n",
            "    \"LABEL_85\": 85,\n",
            "    \"LABEL_86\": 86,\n",
            "    \"LABEL_87\": 87,\n",
            "    \"LABEL_88\": 88,\n",
            "    \"LABEL_89\": 89,\n",
            "    \"LABEL_9\": 9,\n",
            "    \"LABEL_90\": 90,\n",
            "    \"LABEL_91\": 91,\n",
            "    \"LABEL_92\": 92,\n",
            "    \"LABEL_93\": 93,\n",
            "    \"LABEL_94\": 94,\n",
            "    \"LABEL_95\": 95,\n",
            "    \"LABEL_96\": 96,\n",
            "    \"LABEL_97\": 97,\n",
            "    \"LABEL_98\": 98,\n",
            "    \"LABEL_99\": 99\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"xlm-roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.22.2\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 250002\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:416] 2022-10-08 09:08:40,642 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:648] 2022-10-08 09:08:41,573 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/f6d161e8f5f6f2ed433fb4023d6cb34146506b3f/config.json\n",
            "[INFO|configuration_utils.py:700] 2022-10-08 09:08:41,574 >> Model config XLMRobertaConfig {\n",
            "  \"_name_or_path\": \"xlm-roberta-base\",\n",
            "  \"architectures\": [\n",
            "    \"XLMRobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"xlm-roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.22.2\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 250002\n",
            "}\n",
            "\n",
            "Downloading: 100% 5.07M/5.07M [00:01<00:00, 3.06MB/s]\n",
            "Downloading: 100% 9.10M/9.10M [00:02<00:00, 3.88MB/s]\n",
            "[INFO|tokenization_utils_base.py:1773] 2022-10-08 09:08:52,812 >> loading file sentencepiece.bpe.model from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/f6d161e8f5f6f2ed433fb4023d6cb34146506b3f/sentencepiece.bpe.model\n",
            "[INFO|tokenization_utils_base.py:1773] 2022-10-08 09:08:52,812 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/f6d161e8f5f6f2ed433fb4023d6cb34146506b3f/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:1773] 2022-10-08 09:08:52,812 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1773] 2022-10-08 09:08:52,813 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1773] 2022-10-08 09:08:52,813 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:648] 2022-10-08 09:08:52,813 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/f6d161e8f5f6f2ed433fb4023d6cb34146506b3f/config.json\n",
            "[INFO|configuration_utils.py:700] 2022-10-08 09:08:52,814 >> Model config XLMRobertaConfig {\n",
            "  \"_name_or_path\": \"xlm-roberta-base\",\n",
            "  \"architectures\": [\n",
            "    \"XLMRobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"xlm-roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.22.2\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 250002\n",
            "}\n",
            "\n",
            "Downloading: 100% 1.12G/1.12G [00:23<00:00, 47.9MB/s]\n",
            "[INFO|modeling_utils.py:2084] 2022-10-08 09:09:17,790 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/f6d161e8f5f6f2ed433fb4023d6cb34146506b3f/pytorch_model.bin\n",
            "[WARNING|modeling_utils.py:2514] 2022-10-08 09:09:21,194 >> Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.bias', 'roberta.pooler.dense.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:2526] 2022-10-08 09:09:21,194 >> Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Running tokenizer on train dataset:   0% 0/55 [00:00<?, ?ba/s]INFO:datasets.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/multi_eurlex/it/1.0.0/8ec8b79877a517369a143ead6679d1788d13e51cf641ed29772f4449e8364fb6/cache-51ecd5ddba0501d9.arrow\n",
            "Running tokenizer on train dataset: 100% 55/55 [03:34<00:00,  3.90s/ba]\n",
            "INFO:__main__:Sample 8805 of the training set: {'celex_id': '32003D0856', 'text': \"Decisione 2003/856/PESC del Consiglio\\ndell'8 dicembre 2003\\nconcernente l'attuazione dell'azione comune 2002/210/PESC relativa alla missione di polizia dell'Unione europea\\nIL CONSIGLIO DELL'UNIONE EUROPEA,\\nvista l'azione comune 2002/210/PESC del Consiglio, dell'11 marzo 2002, relativa alla missione di polizia dell'Unione europea (EUPM)(1), in particolare l'articolo 9, paragrafo 1, lettera b), ultimo comma, in combinato disposto con l'articolo 23, paragrafo 2, secondo trattino, del trattato sull'Unione europea,\\nconsiderando quanto segue:\\nIl Consiglio è chiamato a decidere il bilancio definitivo per l'esercizio 2004,\\nDECIDE:\\nArticolo 1\\n1. Un importo di 17,5 milioni di EUR per i costi operativi dell'EUPM nel 2004 è finanziato in comune attingendo al bilancio comunitario.\\n2. La gestione delle spese finanziate dal bilancio comunitario specificate al paragrafo 1 è soggetta alle procedure e alle norme comunitarie che si applicano alle questioni di bilancio, salvo che il prefinanziamento rimanga di proprietà della Comunità.\\nArticolo 2\\nLa presente decisione ha effetto il giorno dell'adozione.\\nArticolo 3\\nLa presente decisione è pubblicata nella Gazzetta ufficiale dell'Unione europea.\\nFatto a Bruxelles, addì 8 dicembre 2003.\", 'labels': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'input_ids': [0, 49132, 15009, 6052, 23538, 13683, 64, 21907, 14495, 146, 74755, 1696, 25, 1019, 41922, 6052, 77422, 1479, 96, 25, 215588, 1696, 25, 6065, 26619, 5726, 64, 88142, 64, 21907, 14495, 70738, 986, 29752, 13, 45, 127424, 1696, 25, 151175, 51547, 30219, 109022, 31343, 127399, 180057, 25, 65761, 31261, 206713, 284, 4, 9462, 96, 25, 6065, 26619, 5726, 64, 88142, 64, 21907, 14495, 146, 74755, 4, 1696, 25, 1662, 18203, 5726, 4, 70738, 986, 29752, 13, 45, 127424, 1696, 25, 151175, 51547, 15, 20214, 20266, 16, 27750, 4, 23, 25642, 96, 25, 74174, 483, 4, 105692, 31, 106, 4, 93640, 876, 247, 109337, 96117, 4, 23, 22321, 188, 113435, 158, 96, 25, 74174, 24645, 105692, 31, 116, 4, 18353, 169438, 157, 4, 146, 22887, 188, 25658, 25, 151175, 51547, 4, 162102, 7045, 32550, 12, 891, 74755, 565, 199375, 10, 177625, 211, 156593, 44836, 31, 117, 96, 25, 132296, 86097, 125508, 36497, 12, 124263, 31, 106, 615, 992, 6, 204191, 45, 106, 109631, 25677, 45, 12220, 117, 17, 54364, 41271, 14, 1696, 25, 20214, 20266, 1718, 4821, 565, 151535, 188, 23, 26619, 243, 214, 4017, 144, 156593, 230170, 5, 787, 239, 40430, 1864, 104995, 151535, 67, 1640, 156593, 230170, 71679, 67, 144, 105692, 31, 106, 565, 221, 69804, 102, 747, 50491, 28, 747, 66800, 6, 235924, 13, 290, 78, 41816, 157, 747, 9655, 14, 45, 156593, 4, 89245, 290, 211, 479, 109459, 14, 6497, 24045, 14201, 45, 81628, 832, 73602, 10552, 5, 124263, 31, 116, 239, 8121, 132956, 256, 6, 98691, 211, 15611, 1696, 25, 1138, 1782, 5, 124263, 31, 138, 239, 8121, 132956, 565, 35706, 102, 3578, 40315, 25643, 11, 112349, 1696, 25, 151175, 51547, 5, 24967, 188, 10, 94961, 4, 15190, 10230, 382, 41922, 46730, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "INFO:__main__:Sample 37303 of the training set: {'celex_id': '32009R0730', 'text': \"REGOLAMENTO (CE) N. 730/2009 DELLA COMMISSIONE\\ndel 10 agosto 2009\\nrecante divieto di pesca dell'aringa nel Mar Baltico, sottodivisioni 22-24 (acque comunitarie) per le navi battenti bandiera polacca\\nLA COMMISSIONE DELLE COMUNITÀ EUROPEE,\\nvisto il trattato che istituisce la Comunità europea,\\nvisto il regolamento (CE) n. 2371/2002 del Consiglio, del 20 dicembre 2002, relativo alla conservazione e allo sfruttamento sostenibile delle risorse della pesca nell'ambito della politica comune della pesca (1), in particolare l'articolo 26, paragrafo 4,\\nvisto il regolamento (CEE) n. 2847/93 del Consiglio, del 12 ottobre 1993, che istituisce un regime di controllo applicabile nell'ambito della politica comune della pesca (2), in particolare l'articolo 21, paragrafo 3,\\nconsiderando quanto segue:\\n(1)\\nIl regolamento (CE) n. 1322/2008 del Consiglio, del 28 novembre 2008, recante fissazione, per il 2009, delle possibilità di pesca e delle condizioni ad esse associate applicabili nel Mar Baltico per alcuni stock o gruppi di stock ittici (3) fissa i contingenti per il 2009.\\n(2)\\nIn base alle informazioni pervenute alla Commissione, le catture dello stock di cui all'allegato del presente regolamento da parte di navi battenti bandiera dello Stato membro ivi indicato o in esso immatricolate hanno determinato l'esaurimento del contingente assegnato per il 2009.\\n(3)\\nÈ quindi necessario vietare la pesca di detto stock nonché la conservazione a bordo, il trasbordo e lo sbarco di catture da esso prelevate,\\nHA ADOTTATO IL PRESENTE REGOLAMENTO:\\nArticolo 1\\nEsaurimento del contingente\\nIl contingente di pesca assegnato per il 2009 allo Stato membro di cui all'allegato del presente regolamento per lo stock ivi indicato si ritiene esaurito a decorrere dalla data stabilita nello stesso allegato.\\nArticolo 2\\nDivieti\\nLa pesca dello stock di cui all'allegato del presente regolamento da parte di navi battenti bandiera dello Stato membro ivi indicato o in esso immatricolate è vietata a decorrere dalla data stabilita nello stesso allegato. Sono vietati la conservazione a bordo, il trasbordo e lo sbarco di catture provenienti dallo stock in questione effettuate dalle navi suddette dopo tale data.\\nArticolo 3\\nEntrata in vigore\\nIl presente regolamento entra in vigore il giorno successivo alla pubblicazione nella Gazzetta ufficiale dell'Unione europea.\\nIl presente regolamento è obbligatorio in tutti i suoi elementi e direttamente applicabile in ciascuno degli Stati membri.\\nFatto a Bruxelles, il 10 agosto 2009.\", 'labels': [0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'input_ids': [0, 60280, 22323, 118844, 15, 10854, 16, 541, 5, 361, 1197, 40148, 161825, 31436, 230498, 647, 146, 209, 18396, 1877, 456, 43632, 45, 47161, 45, 62297, 1696, 25, 101098, 11, 1718, 1784, 129435, 31, 4, 13626, 97368, 38313, 1039, 33503, 15, 2263, 944, 6, 235924, 13, 16, 117, 95, 24, 686, 1777, 510, 118, 8753, 18298, 4070, 11210, 3358, 31436, 230498, 647, 391, 80020, 169575, 6371, 20376, 206713, 647, 4, 13323, 211, 22887, 188, 290, 17, 13480, 17423, 21, 73602, 10552, 51547, 4, 13323, 211, 196605, 15, 10854, 16, 653, 5, 116, 118552, 131298, 146, 74755, 4, 146, 387, 41922, 5726, 4, 98231, 986, 17467, 1782, 28, 24286, 121821, 2611, 220928, 1864, 87688, 832, 62297, 10076, 25, 51503, 832, 27892, 26619, 832, 62297, 798, 4, 23, 25642, 96, 25, 74174, 29620, 105692, 31, 201, 4, 13323, 211, 196605, 15, 10854, 647, 16, 653, 5, 1372, 13330, 64, 11591, 146, 74755, 4, 146, 427, 50516, 13867, 4, 290, 17, 13480, 17423, 51, 63647, 45, 58563, 41816, 5462, 10076, 25, 51503, 832, 27892, 26619, 832, 62297, 1737, 4, 23, 25642, 96, 25, 74174, 21416, 105692, 31, 138, 4, 162102, 7045, 32550, 12, 798, 891, 196605, 15, 10854, 16, 653, 5, 702, 4015, 57093, 146, 74755, 4, 146, 1372, 15430, 43304, 456, 43632, 110015, 1782, 4, 117, 211, 37771, 1864, 29156, 45, 62297, 28, 1864, 53689, 606, 3533, 30699, 67, 41816, 7180, 1718, 1784, 129435, 31, 117, 23535, 31837, 36, 100204, 45, 31837, 442, 37434, 2788, 110015, 17, 102548, 94769, 117, 211, 20938, 1737, 360, 3647, 747, 21970, 117, 1353, 6743, 986, 134827, 4, 95, 7515, 6644, 18739, 31837, 45, 3109, 756, 25, 13474, 76571, 146, 8121, 196605, 48, 1235, 45, 24, 686, 1777, 510, 118, 8753, 18298, 18739, 61872, 60735, 17, 686, 167817, 36, 23, 6, 33144, 6, 67144, 35297, 19309, 5662, 66885, 31, 96, 25, 25107, 1162, 2611, 146, 158, 1916, 4574, 165366, 188, 117, 211, 20938, 2788, 19443, 14410, 41824, 30995, 107, 21, 62297, 45, 43507, 31837, 126932, 21, 17467, 1782, 10, 73662, 4, 211, 5848, 18492, 31, 28, 459, 91, 1299, 587, 45, 7515, 6644, 48, 6, 33144, 479, 38171, 67, 4, 11666, 19831, 104328, 78863, 30219, 6, 108057, 64018, 60280, 22323, 118844, 12, 124263, 31, 106, 73835, 1162, 2611, 146, 158, 1916, 4574, 891, 158, 1916, 4574, 45, 62297, 165366, 188, 117, 211, 1877, 24286, 61872, 60735, 45, 3109, 756, 25, 13474, 76571, 146, 8121, 196605, 117, 459, 31837, 17, 686, 167817, 78, 1427, 81228, 7970, 1162, 188, 10, 8, 166957, 5400, 2053, 40780, 11, 74810, 19053, 747, 76571, 5, 124263, 31, 116, 76278, 7207, 239, 62297, 18739, 31837, 45, 3109, 756, 25, 13474, 76571, 146, 8121, 196605, 48, 1235, 45, 24, 686, 1777, 510, 118, 8753, 18298, 18739, 61872, 60735, 17, 686, 167817, 36, 23, 6, 33144, 6, 67144, 35297, 19309, 565, 30995, 102, 10, 8, 166957, 5400, 2053, 40780, 11, 74810, 19053, 747, 76571, 5, 23992, 30995, 118, 21, 17467, 1782, 10, 73662, 4, 211, 5848, 18492, 31, 28, 459, 91, 1299, 587, 45, 7515, 6644, 149812, 110594, 31837, 23, 92546, 96438, 67, 14763, 24, 686, 8932, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.\n",
            "INFO:__main__:Sample 52577 of the training set: {'celex_id': '32003R1582', 'text': \"Regolamento (CE) n. 1582/2003 della Commissione\\ndel 10 settembre 2003\\nche rettifica il regolamento (CE) n. 1433/2003 recante modalità di applicazione del regolamento (CE) n. 2200/96 del Consiglio riguardo ai fondi di esercizio, ai programmi operativi e all'aiuto finanziario\\nLA COMMISSIONE DELLE COMUNITÀ EUROPEE,\\nvisto il trattato che istituisce la Comunità europea,\\nvisto il regolamento (CE) n. 2200/96 del Consiglio, del 28 ottobre 1996, relativo all'organizzazione comune dei mercati nel settore degli ortofrutticoli(1), modificato da ultimo dal regolamento (CE) n. 47/2003 della Commissione(2), in particolare l'articolo 48,\\nconsiderando quanto segue:\\n(1) Il regolamento (CE) n. 1433/2003 della Commissione(3) ha fissato all'articolo 28, paragrafo 1, che reca disposizioni transitorie, il termine entro il quale le organizzazioni di produttori devono presentare le modifiche necessarie dei programmi operativi approvati dagli Stati membri prima della data di entrata in vigore del regolamento medesimo e la cui esecuzione si protrae nel 2004.\\n(2) La disposizione relativa al suddetto termine non corrisponde alla misura sottoposta al parere del comitato di gestione. Occorre pertanto rettificare tale data,\\nHA ADOTTATO IL PRESENTE REGOLAMENTO:\\nArticolo 1\\nAll'articolo 28, paragrafo 1, del regolamento (CE) n. 1433/2003, la data del 15 settembre 2003 è sostituita da quella del 15 ottobre 2003.\\nArticolo 2\\nIl presente regolamento entra in vigore il giorno della pubblicazione nella Gazzetta ufficiale dell'Unione europea.\\nIl presente regolamento è obbligatorio in tutti i suoi elementi e direttamente applicabile in ciascuno degli Stati membri.\\nFatto a Bruxelles, il 10 settembre 2003.\", 'labels': [0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'input_ids': [0, 202164, 15, 10854, 16, 653, 5, 423, 12012, 110364, 832, 134827, 146, 209, 48287, 6052, 290, 10267, 61093, 211, 196605, 15, 10854, 16, 653, 5, 616, 9185, 110364, 456, 43632, 67918, 45, 6, 97715, 146, 196605, 15, 10854, 16, 653, 5, 146646, 64, 11648, 146, 74755, 132659, 1300, 71953, 45, 6, 132296, 4, 1300, 56037, 41271, 14, 28, 756, 25, 508, 12762, 95981, 31, 3358, 31436, 230498, 647, 391, 80020, 169575, 6371, 20376, 206713, 647, 4, 13323, 211, 22887, 188, 290, 17, 13480, 17423, 21, 73602, 10552, 51547, 4, 13323, 211, 196605, 15, 10854, 16, 653, 5, 146646, 64, 11648, 146, 74755, 4, 146, 1372, 50516, 11891, 4, 98231, 756, 25, 89531, 26619, 1613, 60080, 14, 1718, 32172, 4938, 140470, 164820, 13886, 150, 27750, 4, 24367, 188, 48, 109337, 1640, 196605, 15, 10854, 16, 653, 5, 7657, 110364, 832, 134827, 40970, 4, 23, 25642, 96, 25, 74174, 4572, 4, 162102, 7045, 32550, 12, 798, 891, 196605, 15, 10854, 16, 653, 5, 616, 9185, 110364, 832, 134827, 69829, 256, 110015, 188, 756, 25, 74174, 29835, 105692, 31, 106, 4, 290, 58520, 192782, 62621, 3929, 13, 4, 211, 32498, 49478, 211, 16543, 95, 218928, 45, 190994, 64677, 18691, 13, 95, 190262, 43216, 13, 1613, 56037, 41271, 14, 35707, 10771, 45919, 71465, 46034, 2829, 832, 2053, 45, 14018, 102, 23, 62502, 13, 146, 196605, 128, 93989, 28, 21, 3109, 6, 182994, 78, 502, 1517, 13, 1718, 44211, 1737, 239, 49332, 70738, 144, 8932, 152803, 32498, 351, 32324, 18908, 112, 986, 64579, 13626, 16063, 144, 82502, 146, 375, 913, 188, 45, 40430, 5, 180, 238, 32399, 162742, 16640, 120971, 9342, 2053, 4, 11666, 19831, 104328, 78863, 30219, 6, 108057, 64018, 60280, 22323, 118844, 12, 124263, 31, 106, 3164, 25, 74174, 29835, 105692, 31, 106, 4, 146, 196605, 15, 10854, 16, 653, 5, 616, 9185, 110364, 4, 21, 2053, 146, 423, 48287, 6052, 565, 221, 13480, 913, 48, 15213, 146, 423, 50516, 46730, 124263, 31, 116, 891, 8121, 196605, 14018, 23, 62502, 13, 211, 15611, 832, 169390, 3578, 40315, 25643, 11, 112349, 1696, 25, 151175, 51547, 5, 891, 8121, 196605, 565, 6, 186241, 31, 23, 4796, 17, 21461, 50422, 28, 56492, 41816, 5462, 23, 108853, 31, 4938, 71465, 46034, 5, 24967, 188, 10, 94961, 4, 211, 209, 48287, 46730, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "Running tokenizer on validation dataset:   0% 0/5 [00:00<?, ?ba/s]INFO:datasets.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/multi_eurlex/it/1.0.0/8ec8b79877a517369a143ead6679d1788d13e51cf641ed29772f4449e8364fb6/cache-1ade5ec8a521a7fd.arrow\n",
            "Running tokenizer on validation dataset: 100% 5/5 [00:26<00:00,  5.26s/ba]\n",
            "Running tokenizer on prediction dataset:   0% 0/5 [00:00<?, ?ba/s]INFO:datasets.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/multi_eurlex/it/1.0.0/8ec8b79877a517369a143ead6679d1788d13e51cf641ed29772f4449e8364fb6/cache-e74c66296c4257a7.arrow\n",
            "Running tokenizer on prediction dataset: 100% 5/5 [00:35<00:00,  7.17s/ba]\n",
            "[INFO|trainer.py:575] 2022-10-08 09:14:05,951 >> Using cuda_amp half precision backend\n",
            "[INFO|trainer.py:745] 2022-10-08 09:14:05,951 >> The following columns in the training set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text, celex_id. If text, celex_id are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "[INFO|trainer.py:1628] 2022-10-08 09:14:05,976 >> ***** Running training *****\n",
            "[INFO|trainer.py:1629] 2022-10-08 09:14:05,976 >>   Num examples = 55000\n",
            "[INFO|trainer.py:1630] 2022-10-08 09:14:05,976 >>   Num Epochs = 2\n",
            "[INFO|trainer.py:1631] 2022-10-08 09:14:05,976 >>   Instantaneous batch size per device = 8\n",
            "[INFO|trainer.py:1632] 2022-10-08 09:14:05,976 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1633] 2022-10-08 09:14:05,976 >>   Gradient Accumulation steps = 2\n",
            "[INFO|trainer.py:1634] 2022-10-08 09:14:05,976 >>   Total optimization steps = 6874\n",
            "{'loss': 0.1341, 'learning_rate': 2.781786441664242e-05, 'epoch': 0.15}\n",
            "{'loss': 0.075, 'learning_rate': 2.5635728833284843e-05, 'epoch': 0.29}\n",
            "{'loss': 0.0633, 'learning_rate': 2.3453593249927262e-05, 'epoch': 0.44}\n",
            " 23% 1611/6874 [19:02<1:02:20,  1.41it/s]"
          ]
        }
      ],
      "source": [
        "!sh /content/lex-glue/scripts/run_eurlex.sh"
      ]
    }
  ]
}